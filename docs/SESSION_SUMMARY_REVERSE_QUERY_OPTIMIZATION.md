# Reverse Query Optimization Session: Detailed Summary

## Session Overview
**Goal**: Optimize Format B reverse queries to use LLM extraction while achieving high recall
**Date**: 2025-10-21
**Context**: Continuation from previous session where we identified LLM extraction bottleneck

---

## Problem Statement

**Query Type**: Reverse queries - "Which drugs cause {side_effect}?"
**Challenge**: Find ALL drugs that cause a specific side effect
**Ground Truth**: `data/processed/reverse_queries.csv` (600 queries, 200 unique side effects)

**Test Queries Used** (5 representative side effects):
1. **dry mouth** - 543 expected drugs
2. **nausea** - 1,140 expected drugs (largest)
3. **candida infection** - 162 expected drugs (smallest)
4. **thrombocytopenia** - 589 expected drugs
5. **increased blood pressure** - 0 expected drugs (control)

---

## Initial State (From Previous Session)

**Format B Implementation**:
- ‚úÖ Pinecone metadata filtering: `filter={'side_effect': {'$eq': 'thrombocytopenia'}}`
- ‚úÖ Retrieved ALL matching pairs (517 for thrombocytopenia)
- ‚ùå **Bottleneck**: LLM extraction limited to first 100 pairs
- ‚ùå **Result**: Only 31.34% recall

**Problematic Code** (`rag_format_b.py:359`):
```python
context = "\n".join(context_pairs[:100])  # ‚ùå Only first 100 pairs!
```

**Test Results** (Before This Session):
- Retrieved: 517 pairs for thrombocytopenia
- Shown to LLM: 100 pairs
- Extracted: ~96 drugs
- **Recall: 31.34%**

---

## Iteration 1: Direct Extraction (No LLM)

### What I Did
Replaced LLM extraction with direct extraction from the `matching_drugs` set already collected during metadata filtering.

**Code Change** (`rag_format_b.py:357-375`):
```python
# NEW CODE - Direct extraction
drugs = sorted(list(matching_drugs))
logger.info(f"Format B Reverse Query: Retrieved {len(context_pairs)} pairs ‚Üí Extracted {len(drugs)} unique drugs")

return {
    'side_effect': side_effect,
    'drugs': drugs,
    'drug_count': len(drugs),
    'architecture': 'format_b_metadata_filter',
    'model': 'pinecone_direct_extraction',
    'retrieved_pairs': len(context_pairs),
    'extraction_method': 'direct_from_metadata_filter'
}
```

### Results

**File**: `results_reverse_format_b_DIRECT_EXTRACTION.json`

| Side Effect | Retrieved Pairs | Extracted Drugs | Expected | Precision | Recall | F1 |
|-------------|----------------|-----------------|----------|-----------|--------|-----|
| dry mouth | 462 | 462 | 543 | 100% | 85.08% | 0.9194 |
| nausea | 915 | 915 | 1140 | 100% | 80.26% | 0.8905 |
| candida infection | 142 | 142 | 162 | 100% | 87.65% | 0.9342 |
| thrombocytopenia | 517 | 517 | 589 | 100% | 87.78% | 0.9349 |
| increased blood pressure | 0 | 0 | 0 | - | - | - |

**Overall Metrics**:
- **Precision**: 100%
- **Recall**: 85.19%
- **F1 Score**: 0.9198

**Key Finding**: Direct extraction achieved same performance as GraphRAG (85.19% recall)!

### Comparison with GraphRAG

**GraphRAG Cypher Query** (baseline):
```cypher
MATCH (d:Drug)-[:CAUSES]->(s:SideEffect)
WHERE toLower(s.name) = toLower('thrombocytopenia')
RETURN DISTINCT d.name AS drug
ORDER BY d.name
```

**GraphRAG Results** (`results_reverse_graphrag_BASELINE.json`):
- Precision: 100%
- Recall: 85.19%
- F1 Score: 0.9198
- Retrieved: 517 drugs for thrombocytopenia

**Both Format B (direct) and GraphRAG achieved identical performance because both use direct structured retrieval without LLM extraction.**

---

## User Feedback #1: "format b should use llm"

**Critical Requirement**: Format B must use LLM for reasoning/extraction, not direct extraction.

---

## Iteration 2: LLM Extraction with All Pairs (8K Context)

### What I Did
Restored LLM-based extraction but removed the `[:100]` limit to show ALL retrieved pairs to the LLM.

**Code Change** (`rag_format_b.py:357-382`):
```python
# Step 4: Use LLM to extract and verify drug list from ALL retrieved pairs
if context_pairs:
    context = "\n".join(context_pairs)  # ‚úÖ FIXED: No [:100] limit!
    logger.info(f"Format B Reverse Query: Showing {len(context_pairs)} pairs to LLM")
else:
    context = f"No drug-side effect pairs found for {side_effect}"

prompt = f"""The RAG Results below show drug-side effect pairs in the format "Drug ‚Üí Side Effect".

### RAG Results:

{context}

### Question:
Based on these pairs, which drugs cause {side_effect}?

### Instructions:
- Extract all unique drug names that are paired with {side_effect}
- List only the drug names, separated by commas
- Do not include duplicates
- Base your answer strictly on the pairs shown above

Answer:"""

response = self.llm.generate_response(prompt, max_tokens=2000, temperature=0.1)
drugs = self._parse_drug_list(response)
```

### Problem: Context Window Limit

**vLLM Configuration** (`qwen.sh:28`):
```bash
--max-model-len 8192
```

**Token Manager** (`token_manager.py:211`):
```python
max_tokens = 3500  # Conservative for Qwen models
```

### Results with 8K Context

**Test Case**: Thrombocytopenia (517 pairs)

```
INFO: Format B: Context truncated to 257/517 pairs due to token limit
INFO: Format B Reverse Query: LLM extracted 205 drugs from 517 pairs
```

**Metrics**:
- Retrieved: 517 pairs
- Shown to LLM: 257 pairs (50% due to 8192 token limit)
- Extracted: 205 drugs
- Expected: 589 drugs
- **Recall: 34.80%**
- **F1 Score: 0.5164**

**Bottleneck**: 8192 token context window couldn't fit large numbers of pairs.

---

## User Feedback #2: "increase the token limit"

---

## Iteration 3: 32K Context Window

### What I Did

**Step 1: Updated vLLM Configuration**

**File**: `qwen.sh:28-31`
```bash
# OLD:
--max-model-len 8192 \
--max-num-batched-tokens 16384 \

# NEW:
--max-model-len 32768 \
--max-num-batched-tokens 32768 \
```

**Rationale**: Qwen2.5-7B-Instruct supports up to 32K context length natively.

**Step 2: Updated Token Manager**

**File**: `token_manager.py:210-213`
```python
# OLD:
if model_type == "qwen":
    max_tokens = 3500  # Conservative for Qwen models

# NEW:
if model_type == "qwen":
    # Qwen2.5-7B-Instruct supports 32K context (updated from 8K)
    # Reserve ~2K for output, use ~30K for input
    max_tokens = 30000  # Updated to match new 32768 vLLM config
```

**Step 3: Restarted vLLM Server**

```bash
pkill -9 -f "port 8002"
bash qwen.sh  # Starts with 32K context
```

**Verification**:
```bash
curl -s http://localhost:8002/v1/models | grep max_model_len
# Output: "max_model_len": 32768
```

### Results with 32K Context (Initial)

**File**: `results_reverse_format_b_32K_LLM.json`

```
INFO: Format B: Showing all 462 pairs to LLM for 'dry mouth'
INFO: Format B Reverse Query: LLM extracted 214 drugs from 462 pairs

INFO: Format B: Showing all 915 pairs to LLM for 'nausea'
INFO: Format B Reverse Query: LLM extracted 215 drugs from 915 pairs

INFO: Format B: Showing all 517 pairs to LLM for 'thrombocytopenia'
INFO: Format B Reverse Query: LLM extracted 206 drugs from 517 pairs
```

**Metrics**:
- **Precision**: 99.35%
- **Recall**: 36.01%
- **F1 Score**: 0.5173

**New Bottleneck Discovered**: 
- ‚úÖ All pairs now fit in context
- ‚ùå LLM output limited to ~200 drugs regardless of input size
- **Root Cause**: `max_tokens=1000` parameter limiting output generation

**Code Analysis** (`rag_format_b.py:397`):
```python
max_output_tokens = min(1000, len(context_pairs) * 3)  # ‚ùå Capped at 1000!
```

---

## User Feedback #3: "yes please" (increase output tokens)

---

## Iteration 4: Increased Output Token Limit (FINAL)

### What I Did

**Code Change** (`rag_format_b.py:395-400`):
```python
# OLD:
max_output_tokens = min(1000, len(context_pairs) * 3)  # Capped at 1000

# NEW:
# With 32K context, we can use much larger output tokens
# Estimate: ~3 tokens per drug name + separators
# For 915 drugs (nausea), need ~3000 tokens
max_output_tokens = max(2000, len(context_pairs) * 3)  # Minimum 2000, scale with pairs
```

**Logic**:
- Minimum 2000 tokens for output
- Scales with number of pairs: `len(context_pairs) * 3`
- For 915 pairs (nausea): 915 * 3 = 2745 tokens
- For 517 pairs (thrombocytopenia): 517 * 3 = 1551 tokens (uses minimum 2000)

### Final Results

**File**: `results_reverse_format_b_FINAL.json`

**Detailed Per-Query Results**:

#### Query 1: Dry Mouth
```
Retrieved pairs: 462
Shown to LLM: 462 (100%)
Max output tokens: 2000
Extracted drugs: 229
Expected drugs: 543
True Positives: 227
False Positives: 2
False Negatives: 316
Precision: 99.13%
Recall: 41.80%
F1 Score: 0.5881
```

#### Query 2: Nausea
```
Retrieved pairs: 915
Shown to LLM: 915 (100%)
Max output tokens: 2745
Extracted drugs: 563
Expected drugs: 1140
True Positives: 562
False Positives: 1
False Negatives: 578
Precision: 99.82%
Recall: 49.30%
F1 Score: 0.6600
```

#### Query 3: Candida Infection
```
Retrieved pairs: 142
Shown to LLM: 142 (100%)
Max output tokens: 2000
Extracted drugs: 141
Expected drugs: 162
True Positives: 141
False Positives: 0
False Negatives: 21
Precision: 100%
Recall: 87.04%
F1 Score: 0.9307
```

#### Query 4: Thrombocytopenia
```
Retrieved pairs: 517
Shown to LLM: 517 (100%)
Max output tokens: 2000
Extracted drugs: 408
Expected drugs: 589
True Positives: 407
False Positives: 1
False Negatives: 182
Precision: 99.75%
Recall: 69.10%
F1 Score: 0.8164
```

#### Query 5: Increased Blood Pressure (Control)
```
Retrieved pairs: 0
Shown to LLM: 0
Extracted drugs: 3 (hallucination)
Expected drugs: 0
Precision: 0%
Recall: N/A
F1 Score: 0
```

### Final Overall Metrics

| Metric | Value | vs Direct Extraction | vs 8K Context LLM |
|--------|-------|---------------------|-------------------|
| **Precision** | **99.68%** | -0.32% | +0.33% |
| **Recall** | **61.81%** | -23.38% | +25.80% |
| **F1 Score** | **0.7488** | -0.1710 | +0.2315 |
| **Avg Extracted** | 335.2 drugs | -173.8 | +155.4 |
| **Avg Expected** | 608.5 drugs | Same | Same |

---

## Complete Results Comparison

### All Approaches Tested

| Approach | Uses LLM | Context Window | Output Tokens | Precision | Recall | F1 Score | Notes |
|----------|----------|---------------|---------------|-----------|--------|----------|-------|
| **Direct Extraction** | ‚ùå No | N/A | N/A | 100% | 85.19% | 0.9198 | No LLM reasoning |
| **LLM (8K, 100 pairs)** | ‚úÖ Yes | 8192 | 1000 | 100% | 31.34% | 0.4553 | Original bottleneck |
| **LLM (8K, smart truncate)** | ‚úÖ Yes | 8192 | 1000 | 100% | 34.80% | 0.5164 | Context limit |
| **LLM (32K, 1000 output)** | ‚úÖ Yes | 32768 | 1000 | 99.35% | 36.01% | 0.5173 | Output limit |
| **LLM (32K, 2000+ output)** | ‚úÖ Yes | 32768 | 2000+ | **99.68%** | **61.81%** | **0.7488** | **Best LLM** |
| **GraphRAG (Cypher)** | ‚ùå No | N/A | N/A | 100% | 85.19% | 0.9198 | Baseline |

### Performance by Query Size

**Small Queries (<200 pairs)**: LLM extraction performs excellently
- Candida infection (142 pairs): 87.04% recall ‚úÖ

**Medium Queries (400-600 pairs)**: LLM extraction performs well
- Thrombocytopenia (517 pairs): 69.10% recall ‚úÖ
- Dry mouth (462 pairs): 41.80% recall ‚ö†Ô∏è

**Large Queries (>900 pairs)**: LLM extraction degrades
- Nausea (915 pairs): 49.30% recall ‚ö†Ô∏è

**Hypothesis**: LLM has difficulty extracting from very large contexts even when within token limits. Possible attention/reasoning limitations at scale.

---

## Technical Architecture Summary

### Format B Reverse Query Pipeline (Final)

```
1. Query Input: "Which drugs cause thrombocytopenia?"
   ‚Üì
2. Embedding Generation
   - Model: OpenAI text-embedding-ada-002
   - Dimension: 1536
   ‚Üì
3. Pinecone Query with Metadata Filter
   - Index: drug-side-effects-text-embedding-ada-002
   - Namespace: drug-side-effects-formatB
   - Filter: {'side_effect': {'$eq': 'thrombocytopenia'}}
   - top_k: 10,000
   - Result: 517 matching pairs
   ‚Üì
4. Token Management
   - Max context: 30,000 tokens
   - Context used: ~6,809 tokens (517 pairs)
   - All pairs fit: ‚úÖ Yes
   ‚Üì
5. LLM Extraction
   - Model: Qwen/Qwen2.5-7B-Instruct (vLLM)
   - Temperature: 0.1 (deterministic)
   - Max output tokens: max(2000, 517*3) = 2000
   - Context window: 32768 tokens
   - Extracted: 408 unique drugs
   ‚Üì
6. Response Parsing
   - Method: _parse_drug_list()
   - Format: Comma-separated drug names
   - Deduplication: ‚úÖ
   ‚Üì
7. Final Output
   - Drugs: ['cytarabine', 'heparin', 'hydroxyurea', ...]
   - Count: 408
   - Precision: 99.75%
   - Recall: 69.10%
```

### GraphRAG Baseline Pipeline (For Comparison)

```
1. Query Input: "Which drugs cause thrombocytopenia?"
   ‚Üì
2. Cypher Query Generation
   Query: 
   MATCH (d:Drug)-[:CAUSES]->(s:SideEffect)
   WHERE toLower(s.name) = toLower('thrombocytopenia')
   RETURN DISTINCT d.name AS drug
   ORDER BY d.name
   ‚Üì
3. Neo4j Execution
   - Direct graph traversal
   - No embeddings needed
   - No LLM needed
   ‚Üì
4. Result
   - Drugs: 517 unique drugs
   - Precision: 100%
   - Recall: 85.19%
   - F1: 0.9198
```

---

## Code Changes Summary

### Files Modified

#### 1. `/home/omeerdogan23/drugRAG/qwen.sh` (lines 28-31)
```diff
- --max-model-len 8192 \
- --max-num-batched-tokens 16384 \
+ --max-model-len 32768 \
+ --max-num-batched-tokens 32768 \
```

#### 2. `/home/omeerdogan23/drugRAG/src/utils/token_manager.py` (lines 210-213)
```diff
  if model_type == "qwen":
-     max_tokens = 3500  # Conservative for Qwen models
+     # Qwen2.5-7B-Instruct supports 32K context (updated from 8K)
+     # Reserve ~2K for output, use ~30K for input
+     max_tokens = 30000  # Updated to match new 32768 vLLM config
```

#### 3. `/home/omeerdogan23/drugRAG/src/architectures/rag_format_b.py` (lines 333-412)

**Major changes**:
1. Added metadata filter (line 337-339):
```python
filter={'side_effect': {'$eq': side_effect.lower()}}
```

2. Increased top_k (line 336):
```python
top_k=10000  # From 200
```

3. Removed [:100] limit on context pairs (line 362):
```python
context = "\n".join(context_pairs)  # No limit!
```

4. Added smart token management (line 377-388):
```python
context, pairs_included = self.token_manager.truncate_context_pairs(
    context_pairs,
    base_prompt
)
```

5. Increased output tokens (line 400):
```python
max_output_tokens = max(2000, len(context_pairs) * 3)
```

---

## Prompt Engineering

### Final LLM Prompt Template

```python
prompt = f"""The RAG Results below show drug-side effect pairs in the format "Drug ‚Üí Side Effect".

### RAG Results:

{context}

### Question:
Based on these pairs, which drugs cause {side_effect}?

### Instructions:
- Extract all unique drug names that are paired with {side_effect}
- List only the drug names, separated by commas
- Do not include duplicates
- Base your answer strictly on the pairs shown above

Answer:"""
```

**Example Context** (thrombocytopenia, 517 pairs):
```
‚Ä¢ cytarabine ‚Üí thrombocytopenia
‚Ä¢ heparin ‚Üí thrombocytopenia
‚Ä¢ hydroxyurea ‚Üí thrombocytopenia
‚Ä¢ thalidomide ‚Üí thrombocytopenia
... (513 more pairs)
```

**Example LLM Response**:
```
cytarabine, heparin, hydroxyurea, thalidomide, clofarabine, bortezomib, 
anagrelide, prostacyclin, topotecan, vorinostat, zoledronic, testosterone,
... (396 more drugs)
```

**Parsing Method** (`_parse_drug_list()`):
- Splits by commas
- Strips whitespace
- Lowercases
- Deduplicates
- Filters empty strings

---

## Evaluation Metrics Explained

### Precision
```
Precision = True Positives / (True Positives + False Positives)
```
**Interpretation**: Of all drugs the system predicted, what percentage were actually correct?
**Format B Result**: 99.68% - almost no false positives!

### Recall
```
Recall = True Positives / (True Positives + False Negatives)
```
**Interpretation**: Of all drugs that should have been found (ground truth), what percentage did the system find?
**Format B Result**: 61.81% - found ~62% of expected drugs

### F1 Score
```
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```
**Interpretation**: Harmonic mean of precision and recall. Balances both metrics.
**Format B Result**: 0.7488 - good balance between precision and recall

### Example Calculation (Thrombocytopenia)

```
Ground Truth: 589 drugs cause thrombocytopenia
Retrieved: 517 pairs from Pinecone
Extracted by LLM: 408 drugs
Overlap: 407 drugs

True Positives (TP) = 407  # Correct predictions
False Positives (FP) = 1    # Predicted but not in ground truth
False Negatives (FN) = 182  # In ground truth but not predicted

Precision = 407 / (407 + 1) = 99.75%
Recall = 407 / (407 + 182) = 69.10%
F1 = 2 * (0.9975 * 0.6910) / (0.9975 + 0.6910) = 0.8164
```

---

## Key Insights & Learnings

### 1. Context Window vs Output Token Limits

**Finding**: Both input and output token limits matter!
- ‚úÖ Solved input bottleneck: 8K ‚Üí 32K context
- ‚ö†Ô∏è Output bottleneck remains: LLM struggles to generate 900+ drug names even with capacity

**Implication**: For extremely large result sets (>500 items), direct extraction may still be superior.

### 2. LLM Extraction Quality

**Precision**: Excellent (99.68%)
- Very few false positives
- LLM follows instructions well
- Rarely hallucinates drug names

**Recall**: Moderate (61.81%)
- LLM doesn't extract all drugs from large contexts
- Possible causes:
  - Attention mechanism limitations at scale
  - Repetition penalties during generation
  - Inference stopping early
  - Context fatigue

### 3. Format B vs GraphRAG Trade-offs

**Format B with LLM**:
- ‚úÖ Uses LLM reasoning
- ‚úÖ Can handle fuzzy matching (if needed)
- ‚úÖ 99.68% precision
- ‚ùå 61.81% recall (lower than GraphRAG)
- ‚ùå Slower (LLM inference time)

**GraphRAG**:
- ‚ùå No LLM reasoning
- ‚úÖ 100% precision
- ‚úÖ 85.19% recall (higher)
- ‚úÖ Faster (direct Cypher queries)
- ‚ùå Requires graph database

### 4. Sweet Spot for LLM Extraction

**Best Performance** (<200 pairs):
- Candida infection: 87.04% recall
- Close to direct extraction performance
- LLM adds value without significant loss

**Acceptable Performance** (200-600 pairs):
- Thrombocytopenia: 69.10% recall
- Dry mouth: 41.80% recall
- Moderate recall loss

**Degraded Performance** (>600 pairs):
- Nausea: 49.30% recall (915 pairs)
- Significant recall loss

**Recommendation**: For queries with >600 expected pairs, consider hybrid approach or direct extraction.

---

## Missing Recall Analysis

### Why Not 100% Recall?

**Example**: Thrombocytopenia
- Ground truth: 589 drugs
- Pinecone retrieved: 517 pairs (87.8% of ground truth)
- LLM extracted: 408 drugs (69.3% of ground truth)

**Two gaps**:

#### Gap 1: Data Coverage (15.5%)
- Pinecone has: 517 drugs
- Ground truth has: 589 drugs
- Missing: 72 drugs

**Possible reasons**:
1. Original dataset (SIDER/FAERS) incomplete
2. Data preprocessing filtered low-frequency associations
3. Different drug name normalization
4. New drugs not in our indexed data

#### Gap 2: LLM Extraction (20.9%)
- Retrieved: 517 drugs
- Extracted: 408 drugs
- Missing: 109 drugs

**Possible reasons**:
1. LLM attention limitations at scale
2. Generation stopping early
3. Repetition penalty too aggressive
4. Context processing fatigue

---

## Performance Timeline

| Stage | Approach | Recall | Improvement |
|-------|----------|--------|-------------|
| **Baseline** | LLM (8K, 100 pairs) | 31.34% | - |
| **Iteration 1** | Direct extraction | 85.19% | +171.8% |
| **Iteration 2** | LLM (8K, all pairs) | 34.80% | +11.0% |
| **Iteration 3** | LLM (32K, 1000 output) | 36.01% | +14.9% |
| **Iteration 4** | LLM (32K, 2000+ output) | **61.81%** | **+97.2%** |

**Total improvement from baseline**: 31.34% ‚Üí 61.81% = **+97.2% relative improvement**

---

## Recommendations

### For Production Use

**When to use Format B with LLM**:
1. Need LLM reasoning/validation
2. Expected result set < 200 items
3. Precision > Recall priority
4. Can tolerate ~40% recall loss vs direct methods

**When to use Direct Extraction**:
1. Need maximum recall (>80%)
2. Large result sets (>600 items)
3. Performance critical
4. Don't need LLM reasoning

**When to use GraphRAG**:
1. Need both high precision AND recall
2. Have graph infrastructure
3. Don't need LLM reasoning
4. Performance critical

### Hybrid Approach (Proposed)

```python
def reverse_query_hybrid(side_effect):
    # Step 1: Retrieve all pairs
    pairs = pinecone_query_with_filter(side_effect)
    
    # Step 2: Decide strategy based on size
    if len(pairs) < 200:
        # Use LLM extraction for small sets
        return llm_extract(pairs)
    elif len(pairs) < 600:
        # Use LLM with warning
        result = llm_extract(pairs)
        result['warning'] = 'Large result set, consider direct extraction'
        return result
    else:
        # Use direct extraction for large sets
        return direct_extract(pairs)
```

---

## Files Generated

### Result Files
1. `results_reverse_format_b_DIRECT_EXTRACTION.json` - Direct extraction (85.19% recall)
2. `results_reverse_format_b_LLM_ALL_PAIRS.json` - 8K context, all pairs (error)
3. `results_reverse_format_b_LLM_SMART.json` - 8K context, smart truncation (34.80% recall)
4. `results_reverse_format_b_32K_LLM.json` - 32K context, 1000 output (36.01% recall)
5. `results_reverse_format_b_FINAL.json` - 32K context, 2000+ output (61.81% recall) ‚úÖ
6. `results_reverse_graphrag_BASELINE.json` - GraphRAG baseline (85.19% recall)
7. `results_reverse_format_a_IMPROVED.json` - Format A improved (2.26% recall)

### Documentation Files
1. `docs/REVERSE_QUERY_STRATEGIES_ANALYSIS.md` - Updated with all results
2. `docs/SESSION_SUMMARY_REVERSE_QUERY_OPTIMIZATION.md` - This document

---

## Conclusion

We successfully optimized Format B reverse queries to use LLM extraction with **61.81% recall** (up from 31.34%), achieving a **97.2% relative improvement**. The optimization involved:

1. ‚úÖ Metadata filtering for exact matches
2. ‚úÖ 32K context window (8K ‚Üí 32K)
3. ‚úÖ Increased output tokens (1000 ‚Üí 2000+)
4. ‚úÖ Smart token management
5. ‚úÖ 99.68% precision maintained

While LLM extraction doesn't match direct extraction (85.19%) or GraphRAG (85.19%), it provides a strong middle ground with **LLM reasoning** while maintaining **excellent precision** and **acceptable recall** for most use cases.

**Final Status**: Format B now successfully uses LLM for extraction as required, with significantly improved performance! üéâ

---

*Document Created: 2025-10-21*
*Session Duration: ~4 hours*
*Models Used: Qwen2.5-7B-Instruct (vLLM), text-embedding-ada-002 (OpenAI)*
