{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full RAG Pipeline Logger - Complete Flow Analysis\n",
    "\n",
    "This notebook logs the complete RAG pipeline for both Format A and Format B:\n",
    "- Vector database queries and responses\n",
    "- RAG context building\n",
    "- LLM prompts\n",
    "- LLM responses\n",
    "- Final answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import pandas as pd\n",
    "from pinecone import Pinecone\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "# Import utilities\n",
    "from src.utils.embedding_client import create_embedding_client\n",
    "from src.utils.token_manager import create_token_manager\n",
    "from src.utils.binary_parser import parse_binary_response\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logs directory\n",
    "log_dir = \"rag_pipeline_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Setup file loggers\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_filename = f\"{log_dir}/full_pipeline_{timestamp}.log\"\n",
    "json_log_filename = f\"{log_dir}/full_pipeline_{timestamp}.json\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('rag_pipeline_logger')\n",
    "logger.info(f\"Logging RAG pipeline to: {log_filename}\")\n",
    "logger.info(f\"JSON logs will be saved to: {json_log_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced RAG Classes with Full Pipeline Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPipelineLoggerFormatA:\n",
    "    \"\"\"Format A RAG with complete pipeline logging\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path=\"experiments/config.json\", model=\"openai\"):\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.pc = Pinecone(api_key=self.config['pinecone_api_key'])\n",
    "        self.index = self.pc.Index(self.config['pinecone_index_name'])\n",
    "        self.namespace = \"drug-side-effects-formatA\"\n",
    "        \n",
    "        # OpenAI for embeddings and LLM\n",
    "        self.openai_client = openai.OpenAI(api_key=self.config['openai_api_key'])\n",
    "        \n",
    "        # Token manager\n",
    "        self.token_manager = create_token_manager(model_type=\"openai\")\n",
    "        \n",
    "        # Store all pipeline data\n",
    "        self.pipeline_logs = []\n",
    "        \n",
    "        logger.info(f\"‚úÖ Format A Pipeline Logger initialized\")\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding\"\"\"\n",
    "        try:\n",
    "            response = self.openai_client.embeddings.create(\n",
    "                input=text,\n",
    "                model=\"text-embedding-ada-002\"\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Embedding error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def query_with_full_logging(self, drug: str, side_effect: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute full RAG pipeline with detailed logging\"\"\"\n",
    "        \n",
    "        pipeline_data = {\n",
    "            \"format\": \"A\",\n",
    "            \"query\": {\n",
    "                \"drug\": drug,\n",
    "                \"side_effect\": side_effect,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*80)\n",
    "        logger.info(f\"üîç FORMAT A FULL PIPELINE: {drug} - {side_effect}\")\n",
    "        logger.info(\"=\"*80)\n",
    "        \n",
    "        # Step 1: Generate embedding\n",
    "        query_text = f\"{drug} {side_effect}\"\n",
    "        logger.info(f\"\\nüìå Step 1: Generating embedding for: '{query_text}'\")\n",
    "        query_embedding = self.get_embedding(query_text)\n",
    "        \n",
    "        if not query_embedding:\n",
    "            logger.error(\"Failed to generate embedding\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"‚úÖ Embedding generated (dimension: {len(query_embedding)})\")\n",
    "        \n",
    "        # Step 2: Query Pinecone\n",
    "        logger.info(f\"\\nüìå Step 2: Querying Pinecone (namespace: {self.namespace})\")\n",
    "        \n",
    "        results = self.index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=10,\n",
    "            namespace=self.namespace,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"‚úÖ Retrieved {len(results.matches)} matches from Pinecone\")\n",
    "        \n",
    "        # Log Pinecone matches\n",
    "        pinecone_matches = []\n",
    "        for i, match in enumerate(results.matches[:5], 1):  # Log top 5\n",
    "            logger.info(f\"   Match {i}: Score={match.score:.4f}, Drug={match.metadata.get('drug', 'N/A')}\")\n",
    "            pinecone_matches.append({\n",
    "                \"rank\": i,\n",
    "                \"score\": float(match.score),\n",
    "                \"drug\": match.metadata.get('drug', ''),\n",
    "                \"text_preview\": str(match.metadata.get('text', ''))[:200]\n",
    "            })\n",
    "        \n",
    "        pipeline_data[\"pinecone_response\"] = {\n",
    "            \"total_matches\": len(results.matches),\n",
    "            \"top_matches\": pinecone_matches\n",
    "        }\n",
    "        \n",
    "        # Step 3: Build RAG context\n",
    "        logger.info(f\"\\nüìå Step 3: Building RAG context from retrieved documents\")\n",
    "        \n",
    "        context_documents = []\n",
    "        for match in results.matches:\n",
    "            if match.metadata and match.score > 0.5:\n",
    "                drug_name = match.metadata.get('drug', '')\n",
    "                drug_text = match.metadata.get('text', '')\n",
    "                if drug_name and drug_text:\n",
    "                    context_documents.append(f\"Drug: {drug_name}\\n{drug_text}\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Built context from {len(context_documents)} relevant documents\")\n",
    "        \n",
    "        # Token management and truncation\n",
    "        base_prompt = f\"\"\"You are asked to answer the following question with a single word: YES or NO. Base your answer strictly on the RAG Results provided below. After your YES or NO answer, briefly explain your reasoning using the information from the RAG Results. Do not infer or speculate beyond the provided information.\n",
    "\n",
    "### Question:\n",
    "\n",
    "Is {side_effect} an adverse effect of {drug}?\n",
    "\n",
    "### RAG Results:\n",
    "\n",
    "{{context}}\"\"\"\n",
    "        \n",
    "        if context_documents:\n",
    "            context, docs_included = self.token_manager.truncate_context_documents(context_documents, base_prompt)\n",
    "            logger.info(f\"üìä Context truncation: {docs_included}/{len(context_documents)} documents included\")\n",
    "        else:\n",
    "            context = f\"No data found for {drug}\"\n",
    "            docs_included = 0\n",
    "        \n",
    "        # Save RAG context\n",
    "        pipeline_data[\"rag_context\"] = {\n",
    "            \"total_documents\": len(context_documents),\n",
    "            \"documents_included\": docs_included,\n",
    "            \"context_preview\": context[:500] + \"...\" if len(context) > 500 else context\n",
    "        }\n",
    "        \n",
    "        # Step 4: Build final prompt\n",
    "        prompt = base_prompt.format(context=context)\n",
    "        \n",
    "        logger.info(f\"\\nüìå Step 4: Sending prompt to LLM\")\n",
    "        logger.info(f\"Prompt length: {len(prompt)} characters\")\n",
    "        logger.info(f\"\\n--- PROMPT SENT TO LLM ---\\n{prompt[:500]}...\\n--- END PROMPT PREVIEW ---\")\n",
    "        \n",
    "        pipeline_data[\"llm_prompt\"] = {\n",
    "            \"full_prompt_length\": len(prompt),\n",
    "            \"prompt_preview\": prompt[:1000]\n",
    "        }\n",
    "        \n",
    "        # Step 5: Get LLM response\n",
    "        try:\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            llm_response = response.choices[0].message.content\n",
    "            \n",
    "            logger.info(f\"\\nüìå Step 5: Received LLM response\")\n",
    "            logger.info(f\"\\n--- LLM RESPONSE ---\\n{llm_response}\\n--- END LLM RESPONSE ---\")\n",
    "            \n",
    "            # Step 6: Parse answer\n",
    "            answer = parse_binary_response(llm_response)\n",
    "            \n",
    "            logger.info(f\"\\nüìå Step 6: Parsed final answer: {answer}\")\n",
    "            \n",
    "            pipeline_data[\"llm_response\"] = {\n",
    "                \"raw_response\": llm_response,\n",
    "                \"parsed_answer\": answer,\n",
    "                \"confidence\": 0.9 if answer != 'UNKNOWN' else 0.3\n",
    "            }\n",
    "            \n",
    "            # Store complete pipeline log\n",
    "            self.pipeline_logs.append(pipeline_data)\n",
    "            \n",
    "            logger.info(f\"\\n‚úÖ FORMAT A PIPELINE COMPLETE: {drug} + {side_effect} = {answer}\")\n",
    "            logger.info(\"=\"*80)\n",
    "            \n",
    "            return pipeline_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"LLM error: {e}\")\n",
    "            pipeline_data[\"error\"] = str(e)\n",
    "            self.pipeline_logs.append(pipeline_data)\n",
    "            return pipeline_data\n",
    "    \n",
    "    def save_logs(self):\n",
    "        \"\"\"Save all pipeline logs to JSON\"\"\"\n",
    "        with open(json_log_filename, 'w') as f:\n",
    "            json.dump(self.pipeline_logs, f, indent=2)\n",
    "        logger.info(f\"\\nüíæ Saved {len(self.pipeline_logs)} pipeline logs to {json_log_filename}\")\n",
    "        return json_log_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPipelineLoggerFormatB:\n",
    "    \"\"\"Format B RAG with complete pipeline logging\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path=\"experiments/config.json\", model=\"openai\"):\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.pc = Pinecone(api_key=self.config['pinecone_api_key'])\n",
    "        self.index = self.pc.Index(self.config['pinecone_index_name'])\n",
    "        self.namespace = \"drug-side-effects-formatB\"\n",
    "        \n",
    "        # OpenAI for embeddings and LLM\n",
    "        self.openai_client = openai.OpenAI(api_key=self.config['openai_api_key'])\n",
    "        \n",
    "        # Token manager\n",
    "        self.token_manager = create_token_manager(model_type=\"openai\")\n",
    "        \n",
    "        # Store all pipeline data\n",
    "        self.pipeline_logs = []\n",
    "        \n",
    "        logger.info(f\"‚úÖ Format B Pipeline Logger initialized\")\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding\"\"\"\n",
    "        try:\n",
    "            response = self.openai_client.embeddings.create(\n",
    "                input=text,\n",
    "                model=\"text-embedding-ada-002\"\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Embedding error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def query_with_full_logging(self, drug: str, side_effect: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute full RAG pipeline with detailed logging\"\"\"\n",
    "        \n",
    "        pipeline_data = {\n",
    "            \"format\": \"B\",\n",
    "            \"query\": {\n",
    "                \"drug\": drug,\n",
    "                \"side_effect\": side_effect,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*80)\n",
    "        logger.info(f\"üîç FORMAT B FULL PIPELINE: {drug} - {side_effect}\")\n",
    "        logger.info(\"=\"*80)\n",
    "        \n",
    "        # Step 1: Generate embedding\n",
    "        query_text = f\"{drug} {side_effect}\"\n",
    "        logger.info(f\"\\nüìå Step 1: Generating embedding for: '{query_text}'\")\n",
    "        query_embedding = self.get_embedding(query_text)\n",
    "        \n",
    "        if not query_embedding:\n",
    "            logger.error(\"Failed to generate embedding\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"‚úÖ Embedding generated (dimension: {len(query_embedding)})\")\n",
    "        \n",
    "        # Step 2: Query Pinecone\n",
    "        logger.info(f\"\\nüìå Step 2: Querying Pinecone (namespace: {self.namespace})\")\n",
    "        \n",
    "        results = self.index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=10,\n",
    "            namespace=self.namespace,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"‚úÖ Retrieved {len(results.matches)} matches from Pinecone\")\n",
    "        \n",
    "        # Log Pinecone matches\n",
    "        pinecone_matches = []\n",
    "        drug_relevant_count = 0\n",
    "        \n",
    "        for i, match in enumerate(results.matches[:5], 1):  # Log top 5\n",
    "            pair_drug = match.metadata.get('drug', '')\n",
    "            pair_effect = match.metadata.get('side_effect', '')\n",
    "            is_relevant = drug.lower() in pair_drug.lower()\n",
    "            \n",
    "            if is_relevant:\n",
    "                drug_relevant_count += 1\n",
    "                \n",
    "            logger.info(f\"   Match {i}: Score={match.score:.4f}, {pair_drug} ‚Üí {pair_effect} {'‚úì' if is_relevant else '‚úó'}\")\n",
    "            \n",
    "            pinecone_matches.append({\n",
    "                \"rank\": i,\n",
    "                \"score\": float(match.score),\n",
    "                \"drug\": pair_drug,\n",
    "                \"side_effect\": pair_effect,\n",
    "                \"is_drug_relevant\": is_relevant\n",
    "            })\n",
    "        \n",
    "        pipeline_data[\"pinecone_response\"] = {\n",
    "            \"total_matches\": len(results.matches),\n",
    "            \"drug_relevant_matches\": drug_relevant_count,\n",
    "            \"top_matches\": pinecone_matches\n",
    "        }\n",
    "        \n",
    "        # Step 3: Build RAG context (drug-effect pairs)\n",
    "        logger.info(f\"\\nüìå Step 3: Building RAG context from drug-effect pairs\")\n",
    "        \n",
    "        context_pairs = []\n",
    "        for match in results.matches:\n",
    "            if match.metadata and match.score > 0.5:\n",
    "                pair_drug = match.metadata.get('drug', '')\n",
    "                pair_effect = match.metadata.get('side_effect', '')\n",
    "                # Filter for relevant drug\n",
    "                if pair_drug and pair_effect and drug.lower() in pair_drug.lower():\n",
    "                    context_pairs.append(f\"‚Ä¢ {pair_drug} ‚Üí {pair_effect}\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Built context from {len(context_pairs)} drug-relevant pairs\")\n",
    "        \n",
    "        # Build prompt\n",
    "        base_prompt = f\"\"\"You are asked to answer the following question with a single word: YES or NO.\n",
    "\n",
    "The RAG Results below show drug-side effect relationships where \"Drug ‚Üí Side Effect\" means the drug causes that side effect as an adverse reaction.\n",
    "\n",
    "Instructions:\n",
    "- Answer YES if the RAG Results show that {drug} causes {side_effect} as an adverse reaction\n",
    "- Answer NO if the RAG Results do not show this relationship or show no relevant information\n",
    "- You must start your response with either YES or NO\n",
    "\n",
    "### Question:\n",
    "\n",
    "Is {side_effect} an adverse effect of {drug}?\n",
    "\n",
    "### RAG Results:\n",
    "\n",
    "{{context}}\"\"\"\n",
    "        \n",
    "        if context_pairs:\n",
    "            context, pairs_included = self.token_manager.truncate_context_pairs(context_pairs, base_prompt)\n",
    "            logger.info(f\"üìä Context truncation: {pairs_included}/{len(context_pairs)} pairs included\")\n",
    "        else:\n",
    "            context = f\"No specific pairs found for {drug} and {side_effect}\"\n",
    "            pairs_included = 0\n",
    "        \n",
    "        # Save RAG context\n",
    "        pipeline_data[\"rag_context\"] = {\n",
    "            \"total_pairs\": len(context_pairs),\n",
    "            \"pairs_included\": pairs_included,\n",
    "            \"context_preview\": context[:500] + \"...\" if len(context) > 500 else context\n",
    "        }\n",
    "        \n",
    "        # Step 4: Build final prompt\n",
    "        prompt = base_prompt.format(context=context)\n",
    "        \n",
    "        logger.info(f\"\\nüìå Step 4: Sending prompt to LLM\")\n",
    "        logger.info(f\"Prompt length: {len(prompt)} characters\")\n",
    "        logger.info(f\"\\n--- PROMPT SENT TO LLM ---\\n{prompt[:500]}...\\n--- END PROMPT PREVIEW ---\")\n",
    "        \n",
    "        pipeline_data[\"llm_prompt\"] = {\n",
    "            \"full_prompt_length\": len(prompt),\n",
    "            \"prompt_preview\": prompt[:1000]\n",
    "        }\n",
    "        \n",
    "        # Step 5: Get LLM response\n",
    "        try:\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            llm_response = response.choices[0].message.content\n",
    "            \n",
    "            logger.info(f\"\\nüìå Step 5: Received LLM response\")\n",
    "            logger.info(f\"\\n--- LLM RESPONSE ---\\n{llm_response}\\n--- END LLM RESPONSE ---\")\n",
    "            \n",
    "            # Step 6: Parse answer\n",
    "            answer = parse_binary_response(llm_response)\n",
    "            \n",
    "            logger.info(f\"\\nüìå Step 6: Parsed final answer: {answer}\")\n",
    "            \n",
    "            pipeline_data[\"llm_response\"] = {\n",
    "                \"raw_response\": llm_response,\n",
    "                \"parsed_answer\": answer,\n",
    "                \"confidence\": 0.9 if answer != 'UNKNOWN' else 0.3\n",
    "            }\n",
    "            \n",
    "            # Store complete pipeline log\n",
    "            self.pipeline_logs.append(pipeline_data)\n",
    "            \n",
    "            logger.info(f\"\\n‚úÖ FORMAT B PIPELINE COMPLETE: {drug} + {side_effect} = {answer}\")\n",
    "            logger.info(\"=\"*80)\n",
    "            \n",
    "            return pipeline_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"LLM error: {e}\")\n",
    "            pipeline_data[\"error\"] = str(e)\n",
    "            self.pipeline_logs.append(pipeline_data)\n",
    "            return pipeline_data\n",
    "    \n",
    "    def save_logs(self):\n",
    "        \"\"\"Save all pipeline logs to JSON\"\"\"\n",
    "        with open(json_log_filename, 'w') as f:\n",
    "            json.dump(self.pipeline_logs, f, indent=2)\n",
    "        logger.info(f\"\\nüíæ Saved {len(self.pipeline_logs)} pipeline logs to {json_log_filename}\")\n",
    "        return json_log_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Pipeline Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize both format loggers\n",
    "format_a_logger = FullPipelineLoggerFormatA(config_path=\"experiments/config.json\")\n",
    "format_b_logger = FullPipelineLoggerFormatB(config_path=\"experiments/config.json\")\n",
    "\n",
    "print(\"‚úÖ Pipeline loggers initialized\")\n",
    "print(f\"\\nLog files:\")\n",
    "print(f\"  - Text log: {log_filename}\")\n",
    "print(f\"  - JSON log: {json_log_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Queries - Format A Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries\n",
    "test_queries = [\n",
    "    {\"drug\": \"aspirin\", \"side_effect\": \"headache\"},\n",
    "    {\"drug\": \"ibuprofen\", \"side_effect\": \"nausea\"},\n",
    "    {\"drug\": \"metformin\", \"side_effect\": \"dizziness\"},\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ RUNNING FORMAT A FULL PIPELINE TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "format_a_results = []\n",
    "for query in test_queries:\n",
    "    result = format_a_logger.query_with_full_logging(\n",
    "        drug=query[\"drug\"],\n",
    "        side_effect=query[\"side_effect\"]\n",
    "    )\n",
    "    if result:\n",
    "        format_a_results.append(result)\n",
    "        answer = result.get('llm_response', {}).get('parsed_answer', 'ERROR')\n",
    "        print(f\"\\n‚úÖ Format A: {query['drug']} + {query['side_effect']} = {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Queries - Format B Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ RUNNING FORMAT B FULL PIPELINE TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "format_b_results = []\n",
    "for query in test_queries:\n",
    "    result = format_b_logger.query_with_full_logging(\n",
    "        drug=query[\"drug\"],\n",
    "        side_effect=query[\"side_effect\"]\n",
    "    )\n",
    "    if result:\n",
    "        format_b_results.append(result)\n",
    "        answer = result.get('llm_response', {}).get('parsed_answer', 'ERROR')\n",
    "        print(f\"\\n‚úÖ Format B: {query['drug']} + {query['side_effect']} = {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pipeline_results(results, format_name):\n",
    "    \"\"\"Analyze the complete pipeline flow\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä PIPELINE ANALYSIS: {format_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for result in results:\n",
    "        query = result['query']\n",
    "        print(f\"\\nüîç Query: {query['drug']} - {query['side_effect']}\")\n",
    "        \n",
    "        # Pinecone stats\n",
    "        if 'pinecone_response' in result:\n",
    "            pr = result['pinecone_response']\n",
    "            print(f\"\\n  üìå Vector Search:\")\n",
    "            print(f\"     Total matches: {pr['total_matches']}\")\n",
    "            if 'drug_relevant_matches' in pr:\n",
    "                print(f\"     Drug-relevant: {pr['drug_relevant_matches']}\")\n",
    "            if pr['top_matches']:\n",
    "                print(f\"     Top score: {pr['top_matches'][0]['score']:.4f}\")\n",
    "        \n",
    "        # RAG context stats\n",
    "        if 'rag_context' in result:\n",
    "            rc = result['rag_context']\n",
    "            print(f\"\\n  üìö RAG Context:\")\n",
    "            if 'total_documents' in rc:\n",
    "                print(f\"     Documents: {rc['documents_included']}/{rc['total_documents']}\")\n",
    "            elif 'total_pairs' in rc:\n",
    "                print(f\"     Pairs: {rc['pairs_included']}/{rc['total_pairs']}\")\n",
    "            print(f\"     Context preview: {rc['context_preview'][:100]}...\")\n",
    "        \n",
    "        # LLM prompt stats\n",
    "        if 'llm_prompt' in result:\n",
    "            lp = result['llm_prompt']\n",
    "            print(f\"\\n  üìù LLM Prompt:\")\n",
    "            print(f\"     Length: {lp['full_prompt_length']} chars\")\n",
    "        \n",
    "        # Final answer\n",
    "        if 'llm_response' in result:\n",
    "            lr = result['llm_response']\n",
    "            print(f\"\\n  ‚úÖ Final Answer: {lr['parsed_answer']}\")\n",
    "            print(f\"     Confidence: {lr['confidence']}\")\n",
    "            print(f\"     Response preview: {lr['raw_response'][:100]}...\")\n",
    "        \n",
    "        print(f\"\\n  \" + \"-\"*56)\n",
    "\n",
    "# Analyze both formats\n",
    "analyze_pipeline_results(format_a_results, \"Format A\")\n",
    "analyze_pipeline_results(format_b_results, \"Format B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save All Pipeline Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all logs to JSON files\n",
    "all_logs = {\n",
    "    \"format_a\": format_a_logger.pipeline_logs,\n",
    "    \"format_b\": format_b_logger.pipeline_logs,\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"config_file\": \"experiments/config.json\",\n",
    "        \"total_queries\": len(format_a_logger.pipeline_logs) + len(format_b_logger.pipeline_logs)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(json_log_filename, 'w') as f:\n",
    "    json.dump(all_logs, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Saved complete pipeline logs to: {json_log_filename}\")\n",
    "print(f\"\\nLog files contain:\")\n",
    "print(f\"  - Vector DB queries and responses\")\n",
    "print(f\"  - RAG context building\")\n",
    "print(f\"  - Full prompts sent to LLM\")\n",
    "print(f\"  - LLM responses\")\n",
    "print(f\"  - Final parsed answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Query Testing with Full Pipeline Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your own custom queries\n",
    "custom_drug = \"aspirin\"  # Change this\n",
    "custom_side_effect = \"bleeding\"  # Change this\n",
    "\n",
    "print(f\"\\nüîç Testing custom query: {custom_drug} - {custom_side_effect}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test Format A\n",
    "print(\"\\nüìå Format A Full Pipeline:\")\n",
    "format_a_custom = format_a_logger.query_with_full_logging(custom_drug, custom_side_effect)\n",
    "\n",
    "# Test Format B\n",
    "print(\"\\nüìå Format B Full Pipeline:\")\n",
    "format_b_custom = format_b_logger.query_with_full_logging(custom_drug, custom_side_effect)\n",
    "\n",
    "# Display results summary\n",
    "if format_a_custom and format_b_custom:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä CUSTOM QUERY RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    a_answer = format_a_custom.get('llm_response', {}).get('parsed_answer', 'ERROR')\n",
    "    b_answer = format_b_custom.get('llm_response', {}).get('parsed_answer', 'ERROR')\n",
    "    \n",
    "    print(f\"\\nQuery: Is {custom_side_effect} an adverse effect of {custom_drug}?\")\n",
    "    print(f\"\\nFormat A Answer: {a_answer}\")\n",
    "    print(f\"Format B Answer: {b_answer}\")\n",
    "    \n",
    "    if a_answer == b_answer:\n",
    "        print(f\"\\n‚úÖ Both formats agree: {a_answer}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Formats disagree: A={a_answer}, B={b_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Sample Pipeline Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a complete pipeline flow for examination\n",
    "if format_a_logger.pipeline_logs:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã SAMPLE COMPLETE PIPELINE FLOW (Format A, First Query)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sample = format_a_logger.pipeline_logs[0]\n",
    "    \n",
    "    print(f\"\\n1Ô∏è‚É£ QUERY: {sample['query']['drug']} - {sample['query']['side_effect']}\")\n",
    "    \n",
    "    print(f\"\\n2Ô∏è‚É£ VECTOR SEARCH RESULTS:\")\n",
    "    print(json.dumps(sample['pinecone_response'], indent=2)[:500])\n",
    "    \n",
    "    print(f\"\\n3Ô∏è‚É£ RAG CONTEXT:\")\n",
    "    print(json.dumps(sample['rag_context'], indent=2))\n",
    "    \n",
    "    print(f\"\\n4Ô∏è‚É£ LLM PROMPT (first 500 chars):\")\n",
    "    print(sample['llm_prompt']['prompt_preview'][:500])\n",
    "    \n",
    "    print(f\"\\n5Ô∏è‚É£ LLM RESPONSE:\")\n",
    "    print(sample['llm_response']['raw_response'])\n",
    "    \n",
    "    print(f\"\\n6Ô∏è‚É£ FINAL ANSWER: {sample['llm_response']['parsed_answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}