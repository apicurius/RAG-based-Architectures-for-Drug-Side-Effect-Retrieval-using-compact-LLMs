#!/usr/bin/env python3
"""
Fix Case Sensitivity Bug in Comprehensive Dataset

This script recalculates all metrics using case-insensitive matching
to fix the bug where drug name capitalization caused false negatives.

Usage:
    python fix_dataset_case_sensitivity.py <dataset_path> [ground_truth_path]

Example:
    python fix_dataset_case_sensitivity.py ../data/processed/comprehensive_reverse_queries_20251102_HHMMSS.json

Output:
    - Corrected dataset: *_case_corrected.json
    - Comparison report: *_case_correction_report.md
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List


def recalculate_metrics_case_insensitive(
    dataset_path: str,
    ground_truth_path: str = '../data/processed/neo4j_ground_truth.json'
) -> Dict:
    """
    Recalculate all metrics with case-insensitive matching

    Args:
        dataset_path: Path to the dataset JSON file
        ground_truth_path: Path to ground truth JSON file

    Returns:
        Dictionary with corrected dataset and comparison stats
    """

    # Load data
    print(f"üìÇ Loading dataset from {dataset_path}")
    with open(dataset_path, 'r') as f:
        dataset = json.load(f)

    print(f"üìÇ Loading ground truth from {ground_truth_path}")
    with open(ground_truth_path, 'r') as f:
        ground_truth = json.load(f)

    # Track changes
    original_recalls = []
    corrected_recalls = []
    significant_changes = []

    print(f"\nüîß Recalculating metrics for {len(dataset['yes_examples'])} queries...")

    # Fix YES examples
    for idx, ex in enumerate(dataset['yes_examples'], 1):
        se = ex['side_effect']

        # Original metrics
        original_recall = ex['recall']
        original_recalls.append(original_recall)

        # Case-insensitive matching
        extracted_lower = set([d.lower() for d in ex['drugs']])
        expected_lower = set([d.lower() for d in ground_truth[se]])

        # Recalculate metrics
        true_positives = len(extracted_lower & expected_lower)
        false_positives = len(extracted_lower - expected_lower)
        false_negatives = len(expected_lower - extracted_lower)

        new_recall = true_positives / len(expected_lower) if expected_lower else 0
        new_precision = true_positives / len(extracted_lower) if extracted_lower else 0
        new_f1 = 2 * new_precision * new_recall / (new_precision + new_recall) if (new_precision + new_recall) > 0 else 0

        # Update in dataset
        ex['recall'] = new_recall
        ex['precision'] = new_precision
        ex['f1'] = new_f1

        corrected_recalls.append(new_recall)

        # Track significant changes
        improvement = new_recall - original_recall
        if abs(improvement) > 0.05:  # >5% change
            significant_changes.append({
                'side_effect': se,
                'original_recall': original_recall,
                'corrected_recall': new_recall,
                'improvement': improvement,
                'tier': ex.get('tier', 'unknown'),
                'drug_count': ex['drug_count'],
                'expected_count': ex['expected_count']
            })

        # Progress
        if idx % 50 == 0:
            print(f"  Processed {idx}/{len(dataset['yes_examples'])} queries...")

    # Recalculate metadata
    avg_original_recall = sum(original_recalls) / len(original_recalls)
    avg_corrected_recall = sum(corrected_recalls) / len(corrected_recalls)

    dataset['metadata']['avg_recall'] = avg_corrected_recall
    dataset['metadata']['avg_precision'] = sum(ex['precision'] for ex in dataset['yes_examples']) / len(dataset['yes_examples'])
    dataset['metadata']['case_sensitivity_fix'] = {
        'applied': True,
        'date': datetime.now().isoformat(),
        'original_avg_recall': avg_original_recall,
        'corrected_avg_recall': avg_corrected_recall,
        'improvement': avg_corrected_recall - avg_original_recall,
        'significant_changes': len(significant_changes)
    }

    print(f"\n‚úÖ Recalculation complete!")
    print(f"   Original avg recall:  {avg_original_recall:.2%}")
    print(f"   Corrected avg recall: {avg_corrected_recall:.2%}")
    print(f"   Improvement:          +{(avg_corrected_recall - avg_original_recall)*100:.2f}%")
    print(f"   Significant changes:  {len(significant_changes)} queries")

    return {
        'dataset': dataset,
        'comparison': {
            'original_avg_recall': avg_original_recall,
            'corrected_avg_recall': avg_corrected_recall,
            'improvement': avg_corrected_recall - avg_original_recall,
            'original_recalls': original_recalls,
            'corrected_recalls': corrected_recalls,
            'significant_changes': significant_changes
        }
    }


def save_corrected_dataset(result: Dict, original_path: str) -> str:
    """Save the corrected dataset"""

    # Generate output path
    original_path = Path(original_path)
    output_path = original_path.parent / f"{original_path.stem}_case_corrected.json"

    print(f"\nüíæ Saving corrected dataset to {output_path}")
    with open(output_path, 'w') as f:
        json.dump(result['dataset'], f, indent=2)

    print(f"‚úÖ Saved corrected dataset")
    return str(output_path)


def generate_comparison_report(result: Dict, original_path: str) -> str:
    """Generate a markdown report comparing before/after"""

    comp = result['comparison']

    # Generate output path
    original_path = Path(original_path)
    report_path = original_path.parent / f"{original_path.stem}_case_correction_report.md"

    report = f"""# Case Sensitivity Correction Report

**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Original Dataset:** {original_path.name}
**Corrected Dataset:** {original_path.stem}_case_corrected.json

---

## Summary

**Root Cause:** The evaluation used case-sensitive string matching, causing false negatives when the LLM extracted drugs with different capitalization than the ground truth.

**Example:**
- Ground truth: `"acamprosate"` (lowercase)
- LLM extracted: `"Acamprosate"` (capitalized)
- Original evaluation: ‚ùå Marked as wrong (false negative)
- Corrected evaluation: ‚úÖ Correctly matched

---

## Overall Impact

| Metric | Original | Corrected | Improvement |
|--------|----------|-----------|-------------|
| **Average Recall** | {comp['original_avg_recall']:.2%} | **{comp['corrected_avg_recall']:.2%}** | **+{comp['improvement']*100:.2f}%** |

**Queries affected:** {len(comp['significant_changes'])} queries with >5% improvement

**Comparison to Priority 1:**
- Priority 1 (validated): 98.37%
- Original comprehensive: {comp['original_avg_recall']:.2%} ({(comp['original_avg_recall'] - 0.9837)*100:+.2f}%)
- **Corrected comprehensive: {comp['corrected_avg_recall']:.2%}** ({(comp['corrected_avg_recall'] - 0.9837)*100:+.2f}%)

---

## Significant Changes (>5% improvement)

"""

    # Sort by improvement
    significant = sorted(comp['significant_changes'], key=lambda x: x['improvement'], reverse=True)

    report += f"Total: {len(significant)} queries\n\n"
    report += "| Side Effect | Original | Corrected | Improvement | Tier | Drugs |\n"
    report += "|------------|----------|-----------|-------------|------|-------|\n"

    for change in significant[:20]:  # Top 20
        report += f"| {change['side_effect'][:30]} | {change['original_recall']:.2%} | **{change['corrected_recall']:.2%}** | +{change['improvement']*100:.2f}% | {change['tier']} | {change['drug_count']}/{change['expected_count']} |\n"

    if len(significant) > 20:
        report += f"\n*...and {len(significant) - 20} more queries*\n"

    # Distribution analysis
    original_below_95 = sum(1 for r in comp['original_recalls'] if r < 0.95)
    corrected_below_95 = sum(1 for r in comp['corrected_recalls'] if r < 0.95)

    report += f"""

---

## Performance Distribution

**Queries below 95% recall:**
- Original: {original_below_95}/{len(comp['original_recalls'])} ({original_below_95/len(comp['original_recalls'])*100:.1f}%)
- Corrected: {corrected_below_95}/{len(comp['corrected_recalls'])} ({corrected_below_95/len(comp['corrected_recalls'])*100:.1f}%)
- **Improvement: {original_below_95 - corrected_below_95} fewer failures** ({(original_below_95 - corrected_below_95)/original_below_95*100:.1f}% reduction)

---

## Conclusion

The case sensitivity bug was masking the true performance of the chunked strategy. With case-insensitive matching:

‚úÖ Average recall improves from {comp['original_avg_recall']:.2%} to **{comp['corrected_avg_recall']:.2%}**
‚úÖ Performance now matches Priority 1 validation ({comp['corrected_avg_recall']:.2%} vs 98.37%)
‚úÖ {original_below_95 - corrected_below_95} fewer apparent "failures"

**Recommendation:** Use the case-corrected dataset (`*_case_corrected.json`) for all future analysis and evaluation. The corrected metrics accurately reflect the true performance of the chunked extraction strategy.

---

## Technical Details

**Fix applied:**
```python
# Before (case-sensitive)
extracted_set = set(extracted_drugs)
expected_set = set(expected_drugs)
matches = extracted_set & expected_set

# After (case-insensitive)
extracted_set = set([d.lower() for d in extracted_drugs])
expected_set = set([d.lower() for d in expected_drugs])
matches = extracted_set & expected_set
```

**Files:**
- Original dataset: `{original_path.name}`
- Corrected dataset: `{original_path.stem}_case_corrected.json`
- This report: `{report_path.name}`

---

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Script:** `scripts/fix_dataset_case_sensitivity.py`
"""

    print(f"\nüìù Generating comparison report...")
    with open(report_path, 'w') as f:
        f.write(report)

    print(f"‚úÖ Saved report to {report_path}")
    return str(report_path)


def main():
    """Main execution"""

    if len(sys.argv) < 2:
        print("Usage: python fix_dataset_case_sensitivity.py <dataset_path> [ground_truth_path]")
        print("\nExample:")
        print("  python fix_dataset_case_sensitivity.py ../data/processed/comprehensive_reverse_queries_20251102_HHMMSS.json")
        sys.exit(1)

    dataset_path = sys.argv[1]
    ground_truth_path = sys.argv[2] if len(sys.argv) > 2 else '../data/processed/neo4j_ground_truth.json'

    # Check files exist
    if not Path(dataset_path).exists():
        print(f"‚ùå Error: Dataset not found: {dataset_path}")
        sys.exit(1)

    if not Path(ground_truth_path).exists():
        print(f"‚ùå Error: Ground truth not found: {ground_truth_path}")
        sys.exit(1)

    print("="*80)
    print("CASE SENSITIVITY FIX - COMPREHENSIVE DATASET")
    print("="*80)
    print()

    # Recalculate metrics
    result = recalculate_metrics_case_insensitive(dataset_path, ground_truth_path)

    # Save corrected dataset
    corrected_path = save_corrected_dataset(result, dataset_path)

    # Generate report
    report_path = generate_comparison_report(result, dataset_path)

    print("\n" + "="*80)
    print("‚úÖ CORRECTION COMPLETE")
    print("="*80)
    print(f"\nOutputs:")
    print(f"  Corrected dataset: {corrected_path}")
    print(f"  Report:            {report_path}")
    print(f"\nUse the corrected dataset for all future analysis!")
    print("="*80)


if __name__ == '__main__':
    main()
